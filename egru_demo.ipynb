{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event based GRU was publised as a conference paper at ICLR 2023: \n",
    "\n",
    "**Efficient recurrent architectures through activity sparsity and sparse back-propagation through time (notable-top-25%)**\n",
    "\n",
    "![egru_qr](media/egru_paper_qr.png \"egru_qr\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24326,
     "status": "ok",
     "timestamp": 1720447876873,
     "user": {
      "displayName": "khaleel khan",
      "userId": "09242389570118408279"
     },
     "user_tz": -120
    },
    "id": "wlqNkcoPIyyU",
    "outputId": "d074756e-64e5-4fb2-cd15-c40a7b480376"
   },
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 22295,
     "status": "ok",
     "timestamp": 1720447899137,
     "user": {
      "displayName": "khaleel khan",
      "userId": "09242389570118408279"
     },
     "user_tz": -120
    },
    "id": "umTUC2CULtf3",
    "outputId": "2f3e4963-2fd5-4720-b673-3238bf04dce1"
   },
   "source": [
    "%pip install git+https://github.com/Efficient-Scalable-Machine-Learning/EvNN.git@feature/egru_cell"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1720447899138,
     "user": {
      "displayName": "khaleel khan",
      "userId": "09242389570118408279"
     },
     "user_tz": -120
    },
    "id": "8DgMDZYEIyyW",
    "outputId": "4ac3d827-f598-4a78-91ce-19e4879ffc39"
   },
   "source": [
    "from evnn_pytorch import EGRU"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![EGRUanim](https://github.com/Efficient-Scalable-Machine-Learning/EvNN/raw/main/media/videos/anim/1080p60/EvNNPlot_ManimCE_v0.17.2.gif \"egru\") -->\n",
    "\n",
    "<img src=\"https://github.com/Efficient-Scalable-Machine-Learning/EvNN/raw/main/media/videos/anim/1080p60/EvNNPlot_ManimCE_v0.17.2.gif\" alt=\"egru\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "TU-I_wVcNEzN"
   },
   "source": [
    "# Download and unzip the trained model\n",
    "!wget -q -O download.zip https://datashare.tu-dresden.de/s/jbzaoqFXwCLYHJF/download\n",
    "!unzip -o download.zip"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5w1NsdCyIyyZ"
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S4oIiaaMIyya"
   },
   "source": [
    "# load ascii mapping\n",
    "filename = \"Enwik8/index2word.json\"\n",
    "with open(filename, 'r', encoding='utf-8') as fp:\n",
    "    i2w = json.load(fp)\n",
    "\n",
    "filename = \"Enwik8/word2index.json\"\n",
    "with open(filename, 'r', encoding='utf-8') as fp:\n",
    "    w2i = json.load(fp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hbqnG49lIyya"
   },
   "source": [
    "eos = w2i.pop(\"<eos>\")\n",
    "w2i = {chr(int(c)):i for c,i in w2i.items()}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pDdDu53oIyyc"
   },
   "source": [
    "n_vocab = len(i2w)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xO3YqVJ5Iyyd"
   },
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 ninp: int,\n",
    "                 ntokens: int,\n",
    "                 project: bool = False,\n",
    "                 nemb: Union[None, int] = None,\n",
    "                 dropout: float = 0.0):\n",
    "        \"\"\"\n",
    "        Takes hidden states of RNNs, optionally applies a projection operation and decodes to output tokens\n",
    "        :param ninp: Input dimension\n",
    "        :param ntokens: Number of tokens of the language model\n",
    "        :param project: If True, applies a linear projection onto the embedding dimension\n",
    "        :param nemb: If projection is True, specifies the dimension of the projection\n",
    "        :param dropout: Dropout rate applied to the projector\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        if project:\n",
    "            assert nemb, \"If projection is True, must specify nemb!\"\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.nemb = nemb if nemb else ninp\n",
    "        self.nout = ntokens\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # projector\n",
    "        self.project = project\n",
    "        if project:\n",
    "            self.projection = nn.Linear(ninp, nemb)\n",
    "        else:\n",
    "            self.projection = nn.Identity()\n",
    "\n",
    "        # word embedding decoder\n",
    "        self.decoder = nn.Linear(self.nemb, self.nout)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, ninp = x.shape\n",
    "        if self.project:\n",
    "            x = x.view(-1, ninp)\n",
    "            x = F.relu(self.projection(x))\n",
    "            x = x.view(bs, seq_len, self.nemb)\n",
    "        x = x.view(-1, self.nemb)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Cic03CDAIyye"
   },
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(n_vocab, 400)\n",
    "        self.rnns = nn.ModuleList([\n",
    "        EGRU(400, 800, batch_first=False),\n",
    "        EGRU(800, 800, batch_first=False),\n",
    "        EGRU(800, 800, batch_first=False)]\n",
    "        )\n",
    "        self.decoder = Decoder(ninp=800, ntokens=n_vocab,\n",
    "                               project=True, nemb=400)\n",
    "\n",
    "    def forward(self, x, y_pre=[None]*3, h_pre=[None]*3):\n",
    "        y_new=[]\n",
    "        h_new=[]\n",
    "        x = self.embeddings(x)\n",
    "        x, h, _ = self.rnns[0].step(x.squeeze(0), y_pre[0], h_pre[0])\n",
    "        y_new.append(x.detach().clone())\n",
    "        h_new.append(h.detach().clone())\n",
    "        x, h, _ = self.rnns[1].step(x, y_pre[1], h_pre[1])\n",
    "        y_new.append(x.detach().clone())\n",
    "        h_new.append(h.detach().clone())\n",
    "        x, h, _ = self.rnns[2].step(x, y_pre[2], h_pre[2])\n",
    "        y_new.append(x.detach().clone())\n",
    "        h_new.append(h.detach().clone())\n",
    "\n",
    "        # produce output\n",
    "        x = self.decoder(x.unsqueeze(0))\n",
    "        return x, y_new, h_new"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HHo4VF2hIyyf"
   },
   "source": [
    "model = CharModel().to(device)\n",
    "model.eval()\n",
    "model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sC4EPoGqIyyg"
   },
   "source": [
    "# Generation using the trained model\n",
    "best_model = torch.load(\n",
    "    \"Enwik8/2024-05-16-Enwik8-EGRU-trained/checkpoints/EGRU_best_model.cpt\", map_location=device)\n",
    "model.load_state_dict(best_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a prompt to start the generation\n",
    "prompt = \"William Shakespeare was an English playwright, poet and actor. He is widely regarded as the greatest writer in the English language and the world's pre-eminent \"\n",
    "\n",
    "# convert the prompt into tokens\n",
    "x = [w2i[c] for c in prompt]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ycxj3TTiAz5q"
   },
   "source": [
    "# Process prompt to a torch tensor\n",
    "x = np.reshape(x, (len(x), 1))\n",
    "x = torch.tensor(x, dtype=torch.int, device=device)\n",
    "\n",
    "# initialize EGRU hidden states\n",
    "state = [None]*3\n",
    "internal_state = [None]*3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: Run the model on your prompt\n",
    "To familiarize with the inner workings of the model try to measure it's activity sparsity on your prompt.\n",
    "The code would involve looping over the tokens and updating the hidden states accordingly."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 2: Write a text generator\n",
    "The model produces logits of a distribution over the vocabulary at each time step.\n",
    "To generate text, we can sample from this distribution.\n",
    "First, apply softmax to the logits, and then sample from the distribution.\n",
    "Usual strategies involve sampling the most likely token greedily. \n",
    "However, sampling from the distribution with a temperature parameter can produce more diverse text."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
